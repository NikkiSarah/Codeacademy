{"cells":[{"source":"# **K-Nearest Neighbours**\n\n## Introduction\n\nBefore diving into the K-Nearest Neighbours algorithm, let’s first take a minute to think about an example.\n\nConsider a dataset of movies. Let’s brainstorm some features of a movie data point. A feature is a piece of information associated with a data point. Here are some potential features of movie data points:\n- the *length* of the movie in minutes.\n- the *budget* of a movie in dollars.\n\nIf you think back to the previous exercise, you could imagine movies being places in that two-dimensional space based on those numeric features. There could also be some boolean features: features that are either true or false. For example, here are some potential boolean features:\n- *Black and white*. This feature would be `True` for black and white movies and `False` otherwise.\n- *Directed by Stanley Kubrick*. This feature would be `False` for almost every movie, but for the few movies that were directed by Kubrick, it would be `True`.\n\nFinally, let’s think about how we might want to classify a movie. For the rest of this lesson, we’re going to be classifying movies as either good or bad. In our dataset, we’ve classified a movie as good if it had an IMDb rating of 7.0 or greater. Every “good” movie will have a class of `1`, while every bad movie will have a class of `0`.","metadata":{},"cell_type":"markdown","id":"63904a49-a5d2-43ae-9f20-383f4d4d53d6"},{"source":"## Distance Between Points - 2D\n\nWe need to define what it means for two points to be close together or far apart. To do this, we’re going to use the Distance Formula.\n\nFor this example, the data has two dimensions:\n- The length of the movie\n- The movie’s release date\n\nConsider *Star Wars* and *Raiders of the Lost Ark*. *Star Wars* is 125 minutes long and was released in 1977. *Raiders of the Lost Ark* is 115 minutes long and was released in 1981.\n\nThe distance between the movies is computed below:\n$$\\sqrt{(125 - 115)^2 + (1977 - 1981)^2} = 10.77$$","metadata":{},"cell_type":"markdown","id":"6e070490-e190-4766-9594-7b090a190820"},{"source":"## Distance Between Points - 3D\n\nMaking a movie rating predictor based on just the length and release date of movies is pretty limited. There are so many more interesting pieces of data about movies that we could use! So let’s add another dimension.\n\nLet’s say this third dimension is the movie’s budget. We now have to find the distance between these two points in three dimensions.\n\n<img src = \"3d_graph_img.png\" height=40% width=40%/>\n\nWhat if we’re not happy with just three dimensions? Unfortunately, it becomes pretty difficult to visualize points in dimensions higher than 3. But that doesn’t mean we can’t find the distance between them.\n\nThe generalised distance formula between points A and B is as follows:\n$$\\sqrt{(A_1-B_1)^2+(A_2-B_2)^2+ \\dots+(A_n-B_n)^2}$$\n\nHere, A1-B1 is the difference between the first feature of each point. An-Bn is the difference between the last feature of each point.\n\nUsing this formula, we can find the K-Nearest Neighbours of a point in N-dimensional space! We now can use as much information about our movies as we want.\n\nWe will eventually use these distances to find the nearest neighbours to an unlabelled point.","metadata":{},"cell_type":"markdown","id":"4ef7c329-7222-44f1-bdb2-65eede9297bf"},{"source":"## Data with Different Scales - Normalisation\n\nIn the next three lessons, we’ll implement the three steps of the K-Nearest Neighbour Algorithm:\n- **Normalise the data**\n- Find the `k` nearest neighbours\n- Classify the new point based on those neighbours\n\nWhen we added the dimension of budget, you might have realised there are some problems with the way our data currently looks.\n\nConsider the two dimensions of release date and budget. The maximum difference between two movies’ release dates is about 125 years (The Lumière Brothers were making movies in the 1890s). However, the difference between two movies’ budget can be millions of dollars.\n\nThe problem is that the distance formula treats all dimensions equally, regardless of their scale. If two movies came out 70 years apart, that should be a pretty big deal. However, right now, that’s exactly equivalent to two movies that have a difference in budget of 70 dollars. The difference in one year is exactly equal to the difference in one dollar of budget. That’s absurd!\n\nAnother way of thinking about this is that the budget completely outweighs the importance of all other dimensions because it is on such a huge scale. The fact that two movies were 70 years apart is essentially meaningless compared to the difference in millions in the other dimension.\n\nThe solution to this problem is to normalise the data so every value is between 0 and 1. In this lesson, we’re going to be using min-max normalisation.\n\n-----\n\nIt is unlikely, but possible, that the minimum and maximum values for some feature are the same value. In that case, our calculation for normalisation would fail, due to division by sero.\n\n$$\\frac{value-minimum}{maximum-minimum}$$\n\nTo account for this possibility, one thing you can do is skip the calculation, and instead set all the values of that feature to the same value, say 0 or 1, for each data point. This way, they will all be weighed the same.\n\nHowever, we may determine that when all values are the same, then this does not provide any useful information to us. So, we might also consider excluding that feature entirely. For example, say that we had a dataset for animal physical features and that every animal in our dataset had two legs. Since we know that each animal has two legs, then we might exclude that feature in our calculations.","metadata":{},"cell_type":"markdown","id":"a7d2d89d-643f-42b7-b726-3d87a4831c2d"},{"source":"release_dates = [1897, 1998, 2000, 1948, 1962, 1950, 1975, 1960, 2017, 1937, 1968, 1996, 1944, 1891, 1995, 1948, 2011, 1965, 1891, 1978]\n\ndef min_max_normalise(lst):\n    minimum = min(lst)\n    maximum = max(lst)\n\n    normalised = []\n    for i in range(len(lst)):\n        new_val  = (lst[i] - minimum) / (maximum - minimum)\n        normalised.append(new_val)\n\n    return normalised\n\nprint(min_max_normalise(release_dates))","metadata":{},"cell_type":"code","id":"4fe86f01-6a2c-427c-acdf-92a4392bff6c","execution_count":15,"outputs":[{"name":"stdout","text":"[0.047619047619047616, 0.8492063492063492, 0.8650793650793651, 0.4523809523809524, 0.5634920634920635, 0.46825396825396826, 0.6666666666666666, 0.5476190476190477, 1.0, 0.36507936507936506, 0.6111111111111112, 0.8333333333333334, 0.42063492063492064, 0.0, 0.8253968253968254, 0.4523809523809524, 0.9523809523809523, 0.5873015873015873, 0.0, 0.6904761904761905]\n","output_type":"stream"}]},{"source":"## Finding the Nearest Neighbours\n\nThe K-Nearest Neighbour Algorithm:\n- Normalise the data\n- **Find the `k` nearest neighbours**\n- Classify the new point based on those neighbours\n\nNow that our data has been normalised and we know how to find the distance between two points, we can begin classifying unknown data!\n\nTo do this, we want to find the `k` nearest neighbours of the unclassified point. In a few exercises, we’ll learn how to properly choose `k`, but for now, let’s choose a number that seems somewhat reasonable. Let’s choose 5.\n\nIn order to find the 5 nearest neighbours, we need to compare this new unclassified movie to every other movie in the dataset. This means we’re going to be using the distance formula again and again. We ultimately want to end up with a sorted list of distances and the movies associated with those distances.\n\nIt might look something like this:\n```\n[\n  [0.30, 'Superman II'],\n  [0.31, 'Finding Nemo'],\n  ...\n  ...\n  [0.38, 'Blazing Saddles']\n] ```\n\nIn this example, the unknown movie has a distance of 0.30 to Superman II. ","metadata":{},"cell_type":"markdown","id":"7d078a66-a6dd-4eda-819b-67ee43e8776c"},{"source":"import json\n\nf = open('movie_labels.json')\nmovie_labels = json.load(f)\n\nf = open('movie_dataset.json')\nmovie_dataset = json.load(f)","metadata":{},"cell_type":"code","id":"37b90f59-0516-4874-a875-e68feb74e7af","execution_count":5,"outputs":[]},{"source":"def distance(movie1, movie2):\n    squared_difference = 0\n    for i in range(len(movie1)):\n        squared_difference += (movie1[i] - movie2[i]) ** 2\n        distance = squared_difference ** 0.5\n    return distance\n\ndef classify(unknown, dataset, k):\n    distances = []\n\n    for title in dataset:\n        movie_data = dataset[title]\n        distance_to_point = distance(movie_data, unknown)\n        distances.append([distance_to_point, title])\n\n    distances.sort()\n\n    neighbours = distances[:k]\n\n    return neighbours\n\nclassify([0.4, 0.2, 0.9], movie_dataset, 5)","metadata":{},"cell_type":"code","id":"b4e580d5-c271-4a7a-8a8c-40e12e70d4ab","execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"[[0.08273614694606074, 'Lady Vengeance'],\n [0.22989623153818367, 'Steamboy'],\n [0.23641372358159884, 'Fateless'],\n [0.26735445689589943, 'Princess Mononoke'],\n [0.3311022951533416, 'Godzilla 2000']]"},"metadata":{}}]},{"source":"## Count Neighbours\n\nThe K-Nearest Neighbour Algorithm:\n- Normalise the data\n- **Find the `k` nearest neighbours**\n- Classify the new point based on those neighbours\n\nWe’ve now found the `k` nearest neighbours, and have stored them in a list that looks like this:\n```\n[\n  [0.083, 'Lady Vengeance'],\n  [0.236, 'Steamboy'],\n  ...\n  ...\n  [0.331, 'Godzilla 2000']\n]\n```\n\nOur goal now is to count the number of good movies and bad movies in the list of neighbours. If more of the neighbours were good, then the algorithm will classify the unknown movie as good. Otherwise, it will classify it as bad.\n\nIn order to find the class of each of the labels, we’ll need to look at our `movie_labels` dataset. For example, `movie_labels['Akira']` would give us `1` because Akira is classified as a good movie.\n\nYou may be wondering what happens if there’s a tie. What if `k = 8` and four neighbours were good and four neighbours were bad? There are different strategies, but one way to break the tie would be to choose the class of the closest point.","metadata":{},"cell_type":"markdown","id":"cae43ccb-a367-4b81-87b8-b758dce4305e"},{"source":"def classify(unknown, dataset, labels, k):\n    distances = []\n\n    for title in dataset:\n        movie_data = dataset[title]\n        distance_to_point = distance(movie_data, unknown)\n        distances.append([distance_to_point, title])\n\n    distances.sort()\n\n    neighbours = distances[:k]\n    \n    num_good = 0\n    num_bad = 0\n\n    for movie in neighbours:\n        title = movie[1]\n        \n        if labels[title] == 1:\n            num_good += 1\n        else:\n            num_bad += 1\n    \n    if num_good > num_bad:\n        return 1\n    else:\n        return 0\n\nclassify([0.4, 0.2, 0.9], movie_dataset, movie_labels, 5)","metadata":{},"cell_type":"code","id":"43b8ee34-623a-46a8-9820-c8e3d3d18814","execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}]},{"source":"## Classify Your Favorite Movie\n\nNice work! Your classifier is now able to predict whether a movie will be good or bad. So far, we’ve only tested this on a completely random point [.4, .2, .9]. In this exercise we’re going to pick a real movie, normalize it, and run it through our classifier to see what it predicts!\n\nWe are going to be testing our classifier using the 2020 movie *The Call Of The Wild*.","metadata":{},"cell_type":"markdown","id":"62d490ba-eeab-444e-a291-0cd0b72808bc"},{"source":"movie_title = 'The Call Of The Wild'\nmovie_data = [150000, 100, 2020]\n# check that the movie is not already in the dataset\nprint(movie_title in movie_dataset)\n\nnormalised_data = min_max_normalise(movie_data)\nprint(normalised_data)\n\nprint(classify(normalised_data, movie_dataset, movie_labels, 5))","metadata":{},"cell_type":"code","id":"22d99072-82ce-4879-a445-67ad8223f439","execution_count":19,"outputs":[{"name":"stdout","text":"False\n[1.0, 0.0, 0.012808539026017345]\n1\n","output_type":"stream"}]},{"source":"## Training and Validation Sets\n\nYou’ve now built your first K Nearest Neighbours algorithm capable of classification. You can feed your program a never-before-seen movie and it can predict whether its IMDb rating was above or below 7.0. However, we’re not done yet. We now need to report how effective our algorithm is. After all, it’s possible our predictions are totally wrong!\n\nAs with most machine learning algorithms, we have split our data into a training set and validation set.\n\nOnce these sets are created, we will want to use every point in the validation set as input to the K Nearest Neighbour algorithm. We will take a movie from the validation set, compare it to all the movies in the training set, find the K Nearest Neighbours, and make a prediction. After making that prediction, we can then peek at the real answer (found in the validation labels) to see if our classifier got the answer correct.\n\nIf we do this for every movie in the validation set, we can count the number of times the classifier got the answer right and the number of times it got it wrong. Using those two numbers, we can compute the validation accuracy.\n\nValidation accuracy will change depending on what K we use. In the next exercise, we’ll use the validation accuracy to pick the best possible K for our classifier.","metadata":{},"cell_type":"markdown","id":"8f4d830d-2345-4812-8b48-4c2ecb38c525"},{"source":"f = open('training_labels.json')\ntraining_labels = json.load(f)\n\nf = open('training_set.json')\ntraining_dataset = json.load(f)\n\nf = open('validation_labels.json')\nvalidation_labels = json.load(f)\n\nf = open('validation_set.json')\nvalidation_dataset = json.load(f)","metadata":{},"cell_type":"code","id":"c94067ff-98ee-4f3d-98f9-2995c69a9a48","execution_count":50,"outputs":[]},{"source":"print(validation_dataset['Bee Movie'])\nprint(validation_labels['Bee Movie'])\n\nguess = classify(validation_dataset['Bee Movie'], training_dataset, training_labels, 5)\n\nif guess == validation_labels['Bee Movie']:\n    print(\"Correct!\")\nelse:\n    print(\"Wrong!\")","metadata":{},"cell_type":"code","id":"f79b5550-f7af-4254-90ef-c72e30139d80","execution_count":55,"outputs":[{"name":"stdout","text":"[0.012279463360232739, 0.18430034129692832, 0.898876404494382]\n0\nCorrect!\n","output_type":"stream"}]},{"source":"## Choosing K\n\nIn the previous exercise, we found that our classifier got one point in the training set correct. Now we can test every point to calculate the validation accuracy.\n\nThe validation accuracy changes as `k` changes. The first situation that will be useful to consider is when `k` is very small. Let’s say `k = 1`. We would expect the validation accuracy to be fairly low due to *overfitting*. Overfitting is a concept that will appear almost any time you are writing a machine learning algorithm. Overfitting occurs when you rely too heavily on your training data; you assume that data in the real world will always behave exactly like your training data. In the case of K-Nearest Neighbours, overfitting happens when you don’t consider enough neighbours. A single outlier could drastically determine the label of an unknown point. Consider the image below.\n\n<img src = \"dots_img.png\" height=40% width=40%/>\n\nThe dark blue point in the top left corner of the graph looks like a fairly significant outlier. When `k = 1`, all points in that general area will be classified as dark blue when it should probably be classified as green. Our classifier has relied too heavily on the small quirks in the training data.\n\nOn the other hand, if `k` is very large, our classifier will suffer from *underfitting*. Underfitting occurs when your classifier doesn’t pay enough attention to the small quirks in the training set. Imagine you have 100 points in your training set and you set `k = 100`. Every single unknown point will be classified in the same exact way. The distances between the points don’t matter at all! This is an extreme example, however, it demonstrates how the classifier can lose understanding of the training data if `k` is too big.","metadata":{},"cell_type":"markdown","id":"fd09e211-0030-446e-b12a-83fe231a3f53"},{"source":"def find_validation_accuracy(training_set, training_labels, validation_set, validation_labels, k):\n    num_correct = 0.0\n    for title in validation_set:\n        guess = classify(validation_set[title], training_set, training_labels, k)\n\n        if guess == validation_labels[title]:\n            num_correct += 1\n\n    error = num_correct / len(validation_set)\n\n    return error\n\nprint(find_validation_accuracy(training_dataset, training_labels, validation_dataset, validation_labels, k = 3))","metadata":{},"cell_type":"code","id":"b55ee0c5-4fbf-4ffc-8816-bc3e20d19784","execution_count":58,"outputs":[{"name":"stdout","text":"0.6639344262295082\n","output_type":"stream"}]},{"source":"Outliers can affect the classifications in a negative way, because of the sensitivity of K-Nearest Neighbours to them.\n\nEven a single outlier can cause problems if the value of `k` is very small, like `k = 1`, because any point near the outlier will be more influenced by it.\n\nOne reason why outliers are so impactful is that the K-Nearest Neighbours technique is completely dependent upon the input data. Outliers in the input data can impact the boundaries of classification because points that fall near to them can be classified differently than expected.\n\nTo avoid these issues caused by outliers, it can be a good idea to try and remove them initially. Another thing you can do is choose higher values of `k`, larger than 1, but not too large, because this can cause underfitting. By choosing a good value of `k`, it can still remain accurate even despite possible outliers, because it will not only take into account the outlier, but also the surrounding neighbour points.","metadata":{},"cell_type":"markdown","id":"a171d098-6bbb-46e6-807d-c40d23c6ee8e"},{"source":"## Graph of K\n\nThe graph below shows the validation accuracy of our movie classifier as `k` increases. When `k` is small, overfitting occurs and the accuracy is relatively low. On the other hand, when `k` gets too large, underfitting occurs and accuracy starts to drop.\n\n<img src = \"validation_acc_img.png\" height=40% width=40%/>\n\nIn general, yes, any dataset should follow a similar shape as the one shown, although it may appear slightly different.\n- For small values of `k`, the accuracy will be low, because the model will overfit the data.\n- As `k` increases, accuracy will also increase, until eventually reaching a sort of “hump” shape, where the best value of `k` will be between. In this particular graph, this happens around the value `k = 74`, where the validation accuracy is highest.\n- After this “hump”, the accuracy will continue to drop, as `k` increases further, and underfitting occurs due to high `k` values.","metadata":{},"cell_type":"markdown","id":"3efecb9c-83c4-4fae-b30a-cf4d991e79af"},{"source":"## Using sklearn\n\nYou’ve now written your own K-Nearest Neighbour classifier from scratch! However, rather than writing your own classifier every time, you can use Python’s `sklearn` library. `sklearn` is a Python library specifically used for Machine Learning. It has an amazing number of features, but for now, we’re only going to investigate its K-Nearest Neighbour classifier.\n\nThere are a couple of steps we’ll need to go through in order to use the library. First, you need to create a `KNeighborsClassifier` object. This object takes one parameter - `k`. For example, the code below will create a classifier where `k = 3`:\n\n`classifier = KNeighborsClassifier(n_neighbors = 3)`\n\nNext, we’ll need to train our classifier. The `.fit()` method takes two parameters. The first is a list of points, and the second is the labels associated with those points. So for our movie example, we might have something like this\n```\ntraining_points = [\n  [0.5, 0.2, 0.1],\n  [0.9, 0.7, 0.3],\n  [0.4, 0.5, 0.7]\n]\n\ntraining_labels = [0, 1, 1]\nclassifier.fit(training_points, training_labels)\n```\n\nFinally, after training the model, we can classify new points. The `.predict()` method takes a list of points that you want to classify. It returns a list of its guesses for those points.\n```\nunknown_points = [\n  [0.2, 0.1, 0.7],\n  [0.4, 0.7, 0.6],\n  [0.5, 0.8, 0.1]\n]\nguesses = classifier.predict(unknown_points)\n```","metadata":{},"cell_type":"markdown","id":"6e18dc7f-bfb1-474f-9d15-fd761d1aaf3f"},{"source":"import pandas as pd\nfrom random import shuffle, seed\nimport numpy as np\n\nseed(100)\n\ndf = pd.read_csv(\"movies.csv\")\ndf = df.dropna()\n\ngood_movies = df.loc[df['imdb_score'] >= 7]\nbad_movies = df.loc[df['imdb_score'] < 7]\n\ndef min_max_normalise(lst):\n    minimum = min(lst)\n    maximum = max(lst)\n    \n    normalised = []\n    for value in lst:\n        normalised_num = (value - minimum) / (maximum - minimum)\n        normalised.append(normalised_num)\n\n    return normalised\n\nx_good = good_movies[\"budget\"]\ny_good = good_movies[\"duration\"]\nz_good = good_movies['title_year']\nx_bad = bad_movies[\"budget\"]\ny_bad = bad_movies[\"duration\"]\nz_bad = bad_movies['title_year']\n\ndata = [x_good, y_good, z_good, x_bad, y_bad, z_bad]\n\narrays_data = []\nfor d in data:\n    norm_d = min_max_normalise(d)\n    arrays_data.append(np.array(norm_d))\n\ngood_class = list(zip(arrays_data[0].flatten(), arrays_data[1].flatten(), arrays_data[2].flatten(),(np.array(([1] * len(arrays_data[0])))) ))\nbad_class = list(zip(arrays_data[3].flatten(), arrays_data[4].flatten(), arrays_data[5].flatten(),(np.array(([0] * len(arrays_data[0])))) ))\n\ndataset = good_class + bad_class\nshuffle(dataset)\n\nmovie_dataset = []\nlabels = []\nfor movie in dataset:\n    movie_dataset.append(movie[:-1])\n    labels.append(movie[-1])","metadata":{},"cell_type":"code","id":"573eaf65-baf5-4334-92e6-0baebef1f9d0","execution_count":62,"outputs":[]},{"source":"from sklearn.neighbors import KNeighborsClassifier\n\nclassifier = KNeighborsClassifier(n_neighbors=5)\nclassifier.fit(movie_dataset, labels)\n\nprint(classifier.predict([[.45, .2, .5], [.25, .8, .9],[.1, .1, .9]]))","metadata":{},"cell_type":"code","id":"009c73b2-dbed-4285-af65-6cd6125ee05e","execution_count":63,"outputs":[{"name":"stdout","text":"[1 1 0]\n","output_type":"stream"}]},{"source":"## Review\n\nCongratulations! You just implemented your very own classifier from scratch and used Python’s sklearn library. In this lesson, you learned some techniques very specific to the K-Nearest Neighbour algorithm, but some general machine learning techniques as well. Some of the major takeaways from this lesson include:\n- Data with `n` features can be conceptualised as points lying in n-dimensional space.\n- Data points can be compared by using the distance formula. Data points that are similar will have a smaller distance between them.\n- A point with an unknown class can be classified by finding the `k` nearest neighbours.\n- To verify the effectiveness of a classifier, data with known classes can be split into a training set and a validation set. Validation error can then be calculated.\n- Classifiers have parameters that can be tuned to increase their effectiveness. In the case of K-Nearest Neighbours, `k` can be changed.\n- A classifier can be trained improperly and suffer from overfitting or underfitting. In the case of K-Nearest Neighbours, a low `k` often leads to overfitting and a large `k` often leads to underfitting.\n- Python’s `sklearn` library can be used for many classification and machine learning algorithms.","metadata":{},"cell_type":"markdown","id":"4843b322-bb17-45f1-b450-a56baf744612"},{"source":"","metadata":{},"cell_type":"code","id":"ede6ce9b-a894-43fd-9220-64b22be74e55","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":""},"kernelspec":{"display_name":"","name":""}},"nbformat":4,"nbformat_minor":5}