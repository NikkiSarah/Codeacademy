{"cells":[{"source":"# **Random Forests V2**\n\nWe’ve seen that decision trees can be powerful supervised machine learning models. However, they’re not without their weaknesses — decision trees are often prone to overfitting. We’ve discussed some strategies to minimize this problem, like pruning, but sometimes that isn’t enough. We need to find another way to generalize our trees. This is where the concept of a random forest comes in handy.\n\nA random forest is an *ensemble machine learning technique*. A random forest contains many decision trees that all work together to classify new points. When a random forest is asked to classify a new point, the random forest gives that point to each of the decision trees. Each of those trees reports their classification and the random forest returns the most popular classification. It’s like every tree gets a vote, and the most popular classification wins. Some of the trees in the random forest may be overfit, but by making the prediction based on a large number of trees, overfitting will have less of an impact.","metadata":{},"cell_type":"markdown","id":"06485ab6-a64b-41c4-bc56-ea8ba0bf62f0"},{"source":"## Bootstrapping\n\nYou might be wondering how the trees in the random forest get created. After all, right now, our algorithm for creating a decision tree is deterministic — given a training set, the same tree will be made every time. To make a random forest, we use a technique called *bagging*, which is short for *bootstrap aggregating*. This exercise will explain bootstrapping, which is a type of sampling method done with replacement.\n\nHow it works is as follows: every time a decision tree is made, it is created using a different subset of the points in the training set. For example, if our training set had `1000` rows in it, we could make a decision tree by picking `100` of those rows at random to build the tree. This way, every tree is different, but all trees will still be created from a portion of the training data.\n\nIn bootstrapping, we’re doing this process with replacement. Picture putting all `100` rows in a bag and reaching in and grabbing one row at random. After writing down what row we picked, we put that row back in our bag. This means that when we’re picking our `100` random rows, we could pick the same row more than once. In fact, it’s very unlikely, but all `100` randomly picked rows could all be the same row! Because we’re picking these rows with replacement, there’s no need to shrink our bagged training set from `1000` rows to `100`. We can pick `1000` rows at random, and because we can get the same row more than once, we’ll still end up with a unique data set.","metadata":{},"cell_type":"markdown","id":"6b137242-ad51-49eb-bf70-e9d49d106a8b"},{"source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier, RandomForestRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, mean_absolute_error, precision_score, recall_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.tree import DecisionTreeClassifier","metadata":{},"cell_type":"code","id":"7d64caa0-35d5-4326-88de-9a5a24a97a38","execution_count":59,"outputs":[]},{"source":"# import the data\ndrugs = pd.read_csv(\"drug_consumption.csv\")\n# filter out unnecessary columns\ndrugs.drop(\"ID\", inplace=True, axis=1)\ndrugs.drop(drugs.iloc[:, 12:27], inplace=True, axis=1)\ndrugs.drop(drugs.iloc[:, -3:], inplace=True, axis=1)\ndrugs.head()","metadata":{},"cell_type":"code","id":"58511634-d487-4a70-9a4f-3976790dc32c","execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"application/com.datacamp.data-table.v1+json":{"table":{"schema":{"fields":[{"name":"index","type":"integer"},{"name":"Age","type":"string"},{"name":"Gender","type":"string"},{"name":"Education","type":"string"},{"name":"Country","type":"string"},{"name":"Ethnicity","type":"string"},{"name":"Nscore","type":"number"},{"name":"Escore","type":"number"},{"name":"Oscore","type":"number"},{"name":"AScore","type":"number"},{"name":"Cscore","type":"number"},{"name":"Impulsive","type":"number"},{"name":"SS","type":"number"},{"name":"Mushrooms","type":"string"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":[{"index":0,"Age":"25-34","Gender":"M","Education":"Doctorate degree","Country":"UK","Ethnicity":"White","Nscore":-0.67825,"Escore":1.93886,"Oscore":1.43533,"AScore":0.76096,"Cscore":-0.14277,"Impulsive":-0.71126,"SS":-0.21575,"Mushrooms":"CL0"},{"index":1,"Age":"35-44","Gender":"M","Education":"Professional certificate/ diploma","Country":"UK","Ethnicity":"White","Nscore":-0.46725,"Escore":0.80523,"Oscore":-0.84732,"AScore":-1.6209,"Cscore":-1.0145,"Impulsive":-1.37983,"SS":0.40148,"Mushrooms":"CL1"},{"index":2,"Age":"18-24","Gender":"F","Education":"Masters degree","Country":"UK","Ethnicity":"White","Nscore":-0.14882,"Escore":-0.80615,"Oscore":-0.01928,"AScore":0.59042,"Cscore":0.58489,"Impulsive":-1.37983,"SS":-1.18084,"Mushrooms":"CL0"},{"index":3,"Age":"35-44","Gender":"F","Education":"Doctorate degree","Country":"UK","Ethnicity":"White","Nscore":0.73545,"Escore":-1.6334,"Oscore":-0.45174,"AScore":-0.30172,"Cscore":1.30612,"Impulsive":-0.21712,"SS":-0.21575,"Mushrooms":"CL2"},{"index":4,"Age":"65+","Gender":"F","Education":"Left school at 18 years","Country":"Canada","Ethnicity":"White","Nscore":-0.67825,"Escore":-0.30033,"Oscore":-1.55521,"AScore":2.03972,"Cscore":1.63088,"Impulsive":-1.37983,"SS":-1.54858,"Mushrooms":"CL0"}]},"total_rows":5,"truncation_type":null},"text/plain":"     Age Gender  ...       SS Mushrooms\n0  25-34      M  ... -0.21575       CL0\n1  35-44      M  ...  0.40148       CL1\n2  18-24      F  ... -1.18084       CL0\n3  35-44      F  ... -0.21575       CL2\n4    65+      F  ... -1.54858       CL0\n\n[5 rows x 13 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Gender</th>\n      <th>Education</th>\n      <th>Country</th>\n      <th>Ethnicity</th>\n      <th>Nscore</th>\n      <th>Escore</th>\n      <th>Oscore</th>\n      <th>AScore</th>\n      <th>Cscore</th>\n      <th>Impulsive</th>\n      <th>SS</th>\n      <th>Mushrooms</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>25-34</td>\n      <td>M</td>\n      <td>Doctorate degree</td>\n      <td>UK</td>\n      <td>White</td>\n      <td>-0.67825</td>\n      <td>1.93886</td>\n      <td>1.43533</td>\n      <td>0.76096</td>\n      <td>-0.14277</td>\n      <td>-0.71126</td>\n      <td>-0.21575</td>\n      <td>CL0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>35-44</td>\n      <td>M</td>\n      <td>Professional certificate/ diploma</td>\n      <td>UK</td>\n      <td>White</td>\n      <td>-0.46725</td>\n      <td>0.80523</td>\n      <td>-0.84732</td>\n      <td>-1.62090</td>\n      <td>-1.01450</td>\n      <td>-1.37983</td>\n      <td>0.40148</td>\n      <td>CL1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>18-24</td>\n      <td>F</td>\n      <td>Masters degree</td>\n      <td>UK</td>\n      <td>White</td>\n      <td>-0.14882</td>\n      <td>-0.80615</td>\n      <td>-0.01928</td>\n      <td>0.59042</td>\n      <td>0.58489</td>\n      <td>-1.37983</td>\n      <td>-1.18084</td>\n      <td>CL0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>35-44</td>\n      <td>F</td>\n      <td>Doctorate degree</td>\n      <td>UK</td>\n      <td>White</td>\n      <td>0.73545</td>\n      <td>-1.63340</td>\n      <td>-0.45174</td>\n      <td>-0.30172</td>\n      <td>1.30612</td>\n      <td>-0.21712</td>\n      <td>-0.21575</td>\n      <td>CL2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>65+</td>\n      <td>F</td>\n      <td>Left school at 18 years</td>\n      <td>Canada</td>\n      <td>White</td>\n      <td>-0.67825</td>\n      <td>-0.30033</td>\n      <td>-1.55521</td>\n      <td>2.03972</td>\n      <td>1.63088</td>\n      <td>-1.37983</td>\n      <td>-1.54858</td>\n      <td>CL0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"source":"# print the number of rows and the distribution of gender\nnrows = drugs.shape[0]\nprint(nrows) \nprint(f'Distribution of gender in {nrows} rows of data:')\nprint(drugs.Gender.value_counts(normalize=True))","metadata":{},"cell_type":"code","id":"eab6d100-9fa6-4c18-b584-89c8183b581b","execution_count":32,"outputs":[{"name":"stdout","text":"1884\nDistribution of gender in 1884 rows of data:\nM    0.500531\nF    0.499469\nName: Gender, dtype: float64\n","output_type":"stream"}]},{"source":"# create a bootstrapped sample\nboot_sample = drugs.sample(nrows, replace=True)\nprint(f'Distribution of gender in bootstrapped sample data:')\nprint(boot_sample.Gender.value_counts(normalize=True))","metadata":{},"cell_type":"code","id":"51f0aab3-1f25-4d4f-a5ae-b7ecd124fd12","execution_count":33,"outputs":[{"name":"stdout","text":"Distribution of gender in bootstrapped sample data:\nF    0.520701\nM    0.479299\nName: Gender, dtype: float64\n","output_type":"stream"}]},{"source":"# create 1,000 bootstrapped samples\nmale_perc = []\nfor i in range(1000):\n    boot_sample = drugs.sample(nrows, replace=True)\n    value_counts = boot_sample.Gender.value_counts(normalize=True)\n    male_perc.append(value_counts[1])\n\n# view the distribution of the result\nmean_mp = np.mean(male_perc)\nprint(mean_mp)\n\nsns.histplot(male_perc, bins=20)\nplt.xlabel('Male Percentage')\nsns.despine()","metadata":{},"cell_type":"code","id":"5efb700f-1617-4c54-a72c-7212445265ff","execution_count":34,"outputs":[{"name":"stdout","text":"0.4909294055201699\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"source":"## Bagging\n\nRandom forests create different trees using a process known as bagging, which is short for bootstrapped aggregating. As we already covered bootstrapping, the process starts with creating a single decision tree on a bootstrapped sample of data points in the training set. Then after many trees have been made, the results are “aggregated” together. In the case of a classification task, often the aggregation is taking the majority vote of the individual classifiers. For regression tasks, often the aggregation is the average of the individual regressors.\n\nWe will dive into this process for the drug consumption dataset we used in the previous exercise. The dataset has 12 features:\n- age\n- gender\n- education\n- country of current residence\n- ethnicity\n- neuroticism score\n- extraversion score\n- openness to experience score\n- agreeableness score\n- conscientiousness score\n- impulsiveness score\n- sensation seeing score\n\nOur target variable for prediction is `Mushrooms` or whether or not the drug has been used in the last 12 months or not and can either be `0` or `1`.","metadata":{},"cell_type":"markdown","id":"e31cfd9f-5490-488c-9bb0-eaa622063f3d"},{"source":"# map the categorical variables into a numeric representation\nage_map = {\"18-24\": 0, \"25-34\": 1, \"35-44\": 2, \"45-54\": 3, \"55-64\": 4, \"65+\": 5}\neduc_map = {\"Left school before 16 years\": 0, \"Left school at 16 years\": 1, \"Left school at 17 years\": 2, \"Left school at 18 years\": 3,\n            \"Some college or university, no certificate or degree\": 4, \"Professional certificate/ diploma\": 5, \"University degree\": 6, \n            \"Masters degree\": 7, \"Doctorate degree\": 8}\nuser_map = {'CL0': 0, 'CL1': 0, 'CL2': 0, 'CL3': 1, 'CL4': 1, 'CL5': 1, 'CL6': 1}\n\ndicts = [age_map, educ_map, user_map]\nfor col, enc in zip(drugs[['Age', 'Education', 'Mushrooms']], dicts):\n    drugs[col] = drugs[col].map(lambda x: enc.get(x, x))\n\nenc = OrdinalEncoder()\nencoded_cols = enc.fit_transform(drugs[['Gender', 'Country', 'Ethnicity']])\ndrugs[['Gender', 'Country', 'Ethnicity']] = encoded_cols\ndrugs.head()","metadata":{},"cell_type":"code","id":"c06e7834-018b-429f-a46c-ec91b38e1856","execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"application/com.datacamp.data-table.v1+json":{"table":{"schema":{"fields":[{"name":"index","type":"integer"},{"name":"Age","type":"integer"},{"name":"Gender","type":"number"},{"name":"Education","type":"integer"},{"name":"Country","type":"number"},{"name":"Ethnicity","type":"number"},{"name":"Nscore","type":"number"},{"name":"Escore","type":"number"},{"name":"Oscore","type":"number"},{"name":"AScore","type":"number"},{"name":"Cscore","type":"number"},{"name":"Impulsive","type":"number"},{"name":"SS","type":"number"},{"name":"Mushrooms","type":"integer"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":[{"index":0,"Age":1,"Gender":1,"Education":8,"Country":5,"Ethnicity":6,"Nscore":-0.67825,"Escore":1.93886,"Oscore":1.43533,"AScore":0.76096,"Cscore":-0.14277,"Impulsive":-0.71126,"SS":-0.21575,"Mushrooms":0},{"index":1,"Age":2,"Gender":1,"Education":5,"Country":5,"Ethnicity":6,"Nscore":-0.46725,"Escore":0.80523,"Oscore":-0.84732,"AScore":-1.6209,"Cscore":-1.0145,"Impulsive":-1.37983,"SS":0.40148,"Mushrooms":0},{"index":2,"Age":0,"Gender":0,"Education":7,"Country":5,"Ethnicity":6,"Nscore":-0.14882,"Escore":-0.80615,"Oscore":-0.01928,"AScore":0.59042,"Cscore":0.58489,"Impulsive":-1.37983,"SS":-1.18084,"Mushrooms":0},{"index":3,"Age":2,"Gender":0,"Education":8,"Country":5,"Ethnicity":6,"Nscore":0.73545,"Escore":-1.6334,"Oscore":-0.45174,"AScore":-0.30172,"Cscore":1.30612,"Impulsive":-0.21712,"SS":-0.21575,"Mushrooms":0},{"index":4,"Age":5,"Gender":0,"Education":3,"Country":1,"Ethnicity":6,"Nscore":-0.67825,"Escore":-0.30033,"Oscore":-1.55521,"AScore":2.03972,"Cscore":1.63088,"Impulsive":-1.37983,"SS":-1.54858,"Mushrooms":0}]},"total_rows":5,"truncation_type":null},"text/plain":"   Age  Gender  Education  Country  ...   Cscore  Impulsive       SS  Mushrooms\n0    1     1.0          8      5.0  ... -0.14277   -0.71126 -0.21575          0\n1    2     1.0          5      5.0  ... -1.01450   -1.37983  0.40148          0\n2    0     0.0          7      5.0  ...  0.58489   -1.37983 -1.18084          0\n3    2     0.0          8      5.0  ...  1.30612   -0.21712 -0.21575          0\n4    5     0.0          3      1.0  ...  1.63088   -1.37983 -1.54858          0\n\n[5 rows x 13 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Gender</th>\n      <th>Education</th>\n      <th>Country</th>\n      <th>Ethnicity</th>\n      <th>Nscore</th>\n      <th>Escore</th>\n      <th>Oscore</th>\n      <th>AScore</th>\n      <th>Cscore</th>\n      <th>Impulsive</th>\n      <th>SS</th>\n      <th>Mushrooms</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1.0</td>\n      <td>8</td>\n      <td>5.0</td>\n      <td>6.0</td>\n      <td>-0.67825</td>\n      <td>1.93886</td>\n      <td>1.43533</td>\n      <td>0.76096</td>\n      <td>-0.14277</td>\n      <td>-0.71126</td>\n      <td>-0.21575</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1.0</td>\n      <td>5</td>\n      <td>5.0</td>\n      <td>6.0</td>\n      <td>-0.46725</td>\n      <td>0.80523</td>\n      <td>-0.84732</td>\n      <td>-1.62090</td>\n      <td>-1.01450</td>\n      <td>-1.37983</td>\n      <td>0.40148</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>7</td>\n      <td>5.0</td>\n      <td>6.0</td>\n      <td>-0.14882</td>\n      <td>-0.80615</td>\n      <td>-0.01928</td>\n      <td>0.59042</td>\n      <td>0.58489</td>\n      <td>-1.37983</td>\n      <td>-1.18084</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>0.0</td>\n      <td>8</td>\n      <td>5.0</td>\n      <td>6.0</td>\n      <td>0.73545</td>\n      <td>-1.63340</td>\n      <td>-0.45174</td>\n      <td>-0.30172</td>\n      <td>1.30612</td>\n      <td>-0.21712</td>\n      <td>-0.21575</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>1.0</td>\n      <td>6.0</td>\n      <td>-0.67825</td>\n      <td>-0.30033</td>\n      <td>-1.55521</td>\n      <td>2.03972</td>\n      <td>1.63088</td>\n      <td>-1.37983</td>\n      <td>-1.54858</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"source":"# split the data into 75% train and 25% validation\ny = drugs.Mushrooms\nX = drugs.drop('Mushrooms', axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=True, stratify=y, random_state=0)\n\n# fit a decision tree with a max depth of 5\ndt_clf = DecisionTreeClassifier(max_depth=5)\ndt_clf.fit(X_train, y_train)\ndt_clf.score(X_test, y_test)","metadata":{},"cell_type":"code","id":"e32c7521-6aeb-4e66-8909-3a1eca7d874c","execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"0.7664543524416136"},"metadata":{}}]},{"source":"# fit the decision tree on a bootstrapped sample\nidxs = X_train.sample(X_train.shape[0], replace=True, random_state=0).index\ndt_clf.fit(X_train.loc[idxs], y_train.loc[idxs])\ndt_clf.score(X_test, y_test)","metadata":{"tags":[]},"cell_type":"code","id":"dbc614a5-7233-4b63-8a0b-39370c51424b","execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"0.7643312101910829"},"metadata":{}}]},{"source":"# repeat 10 times and extract predictions on the test set\npreds = []\nrandom_state = 0\nfor i in range(10):\n    idxs = X_train.sample(X_train.shape[0], replace=True, random_state=random_state + 1).index\n    \n    dt_clf.fit(X_train.loc[idxs], y_train.loc[idxs])\n    pred = dt_clf.predict(X_test)\n    preds.append(pred)\nba_pred = np.array(preds).mean(0)\n\n# calculate the overall accuracy\nba_accuracy = accuracy_score(y_test, ba_pred >= 0.5)\nba_accuracy","metadata":{},"cell_type":"code","id":"2af9ae82-b050-4671-a68e-f56cb3d8a695","execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"0.7707006369426752"},"metadata":{}}]},{"source":"## Random Forest Feature Selection\n\nIn addition to using bootstrapped samples of our dataset, we can continue to add variety to the ways our trees are created by randomly selecting the features that are used.\n\nWhen we use a decision tree, all the features are used and the split is chosen as the one that increases the information gain the most. While it may seem counter-intuitive, selecting a random subset of features can help in the performance of an ensemble model. In the following example, we will use a random selection of features prior to model building to add additional variance to the individual trees. While an individual tree may perform worse, sometimes the increases in variance can help model performance of the ensemble model as a whole.\n","metadata":{},"cell_type":"markdown","id":"fdddcade-a0d9-44cc-b019-a28d50abae99"},{"source":"# select 4 random features from the 12 available\nrand_features = np.random.choice(X_train.columns, 4)\nprint(rand_features)\n\n# fit a decision tree on this subset and assess model accuracy\ndt_clf = DecisionTreeClassifier()\n\ndt_clf.fit(X_train[rand_features], y_train)\ndt_clf.score(X_test[rand_features], y_test)","metadata":{},"cell_type":"code","id":"8292b70d-87bd-4bc6-b311-269be709fef8","execution_count":39,"outputs":[{"name":"stdout","text":"['Gender' 'Education' 'Impulsive' 'Country']\n","output_type":"stream"},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"0.772823779193206"},"metadata":{}}]},{"source":"# repeat 10 times, using a different subset of features each time\npreds = []\nfor i in range(10):\n    rand_features = np.random.choice(X_train.columns, 4)\n    print(rand_features)\n    \n    dt_clf.fit(X_train[rand_features], y_train)\n    pred = dt_clf.predict(X_test[rand_features])\n    preds.append(pred)\n\n# calculate an aggregate prediction\nprob_preds = np.array(preds).mean(0)\nagg_preds = prob_preds >= 0.5\nagg_accuracy = accuracy_score(y_test, agg_preds)\nagg_accuracy","metadata":{},"cell_type":"code","id":"ee9ac58e-030a-4e13-aee7-51a8d03cc894","execution_count":40,"outputs":[{"name":"stdout","text":"['Ethnicity' 'Impulsive' 'Nscore' 'Education']\n['Country' 'SS' 'Nscore' 'Nscore']\n['Cscore' 'Cscore' 'Education' 'Gender']\n['AScore' 'Education' 'Nscore' 'Impulsive']\n['Education' 'AScore' 'Country' 'Nscore']\n['Oscore' 'Nscore' 'Ethnicity' 'SS']\n['Gender' 'Education' 'Country' 'Age']\n['Education' 'SS' 'Age' 'Age']\n['Cscore' 'Escore' 'AScore' 'Escore']\n['Oscore' 'Cscore' 'Cscore' 'Ethnicity']\n","output_type":"stream"},{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"0.8110403397027601"},"metadata":{}}]},{"source":"## Bagging in `scikit-learn`\n\nThe two steps we walked through above created trees on bootstrapped samples and randomly selecting features. These can be combined together and implemented at the same time! Combining them adds an additional variation to the base learners for the ensemble model. This in turn increases the ability of the model to generalize to new and unseen data, i.e., it minimizes bias and increases variance. Rather than re-doing this process manually, we will use `scikit-learn's` bagging implementation, `BaggingClassifier()`, to do so.\n\nMuch like other models we have used in `scikit-learn`, we instantiate a instance of `BaggingClassifier()` and specify the parameters. The first parameter, base_estimator refers to the machine learning model that is being bagged. In the case of random forests, the base estimator would be a decision tree. We are going to use a decision tree classifier WITH a `max_depth` of 5, this will be instantiated with `BaggingClassifier(DecisionTreeClassifier(max_depth=5))`.\n\nAfter the model has been defined, methods `.fit()`, `.predict()`, `.score()` can be used as expected. Additional hyperparameters specific to bagging include the number of estimators (`n_estimators`) we want to use and the maximum number of features we’d like to keep (`max_features`).\n\nNote: While we have focused on decision tress classifiers (as this is the base learner for a random forest classifier), this procedure of bagging is not specific to decision trees, and in fact can be used for any base classifier or regression model. The `scikit-learn` implementation is generalizable and can be used for other base models!","metadata":{},"cell_type":"markdown","id":"9c10c1ce-d093-492f-b700-496e5d45a9ec"},{"source":"# initialise a bagging classifier with a decision tree as a base classifier\nbag_dt = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=5), n_estimators=10)\n\nbag_dt.fit(X_train, y_train)\npred = bag_dt.predict(X_test)\nbag_accuracy = accuracy_score(y_test, pred)\nbag_accuracy","metadata":{},"cell_type":"code","id":"5d17700f-8711-4e37-8db8-890af42aed9d","execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"0.8067940552016986"},"metadata":{}}]},{"source":"# initialise a bagging classifier with a decision tree as a base classifier\n# restrict the number of features to 10\nbag_dt_10 = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=5), n_estimators=10, max_features=10)\n\nbag_dt_10.fit(X_train, y_train)\npred = bag_dt_10.predict(X_test)\nbag_accuracy = accuracy_score(y_test, pred)\nbag_accuracy","metadata":{},"cell_type":"code","id":"a6c90915-590c-4b20-926f-7caa32dcf2ff","execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"0.8089171974522293"},"metadata":{}}]},{"source":"# change the base estimator to a logistic regression\nbag_lr = BaggingClassifier(base_estimator=LogisticRegression(), n_estimators=10, max_features=10)\n\nbag_lr.fit(X_train, y_train)\npred = bag_lr.predict(X_test)\nbag_accuracy = accuracy_score(y_test, pred)\nbag_accuracy","metadata":{},"cell_type":"code","id":"b62dde5f-504e-4e2f-a50c-6e44bbe99571","execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"0.8089171974522293"},"metadata":{}}]},{"source":"## Train and Predict using `scikit-learn`\n\nNow that we have covered two major ways to combine trees, both in terms of samples and features, we are ready to get to the implementation of random forests! This will be similar to what we covered in the previous exercises, but the random forest algorithm has a slightly different way of randomly choosing features. Rather than choosing a single random set at the onset, each split chooses a different random set.\n\nFor example, when finding which feature to split the data on the first time, we might randomly choose to only consider age, country and education. After splitting the data on the best feature from that subset, we’ll likely want to split again. For this next split, we’ll randomly select three features again to consider. This time those features might be ethnicity, impulsiveness and education. We’ll continue this process until the tree is complete.\n\nOne question to consider is how to choose the number of features to randomly select. Why did we choose 3 in this example? A good rule of thumb is select as many features as the square root of the total number of features. Our drugs dataset doesn’t have a lot of features, so in this example, it’s difficult to follow this rule. But if we had a dataset with 25 features, we’d want to randomly select 5 features to consider at every split point.\n\nYou now have the ability to make a random forest using your own decision trees. However, `scikit-learn` has a `RandomForestClassifier()` class that will do all of this work for you!\n\n`RandomForestClassifier()` works almost identically to `DecisionTreeClassifier()` — the `.fit()`, `.predict()`, and `.score()` methods work in the exact same way.","metadata":{},"cell_type":"markdown","id":"c416fc0d-1ae2-48d5-87a1-04efc6328636"},{"source":"# initialise a random forest classifier and view available parameters/hyperparameters\nrf = RandomForestClassifier()\nprint(rf.get_params())","metadata":{},"cell_type":"code","id":"d633b094-2e2e-446f-a1dc-72cf23d34d54","execution_count":44,"outputs":[{"name":"stdout","text":"{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n","output_type":"stream"}]},{"source":"# fit on the training data and view model accuracy\nrf.fit(X_train, y_train)\npreds = rf.predict(X_test)\naccuracy_score(y_test, preds)","metadata":{},"cell_type":"code","id":"c583cf1d-b3ab-4bd5-8f67-c332c79fa4ef","execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"0.8131634819532909"},"metadata":{}}]},{"source":"# view other performance metrics\nrf_precision = precision_score(y_test, preds)\nrf_recall = recall_score(y_test, preds)\nrf_cm = confusion_matrix(y_test, preds)\n\nprint(f'Model precision: {rf_precision}')\nprint(f'Model recall: {rf_recall}')\nprint('Confusion matrix:')\nprint(rf_cm)","metadata":{},"cell_type":"code","id":"b9b6a5c4-346d-453e-81eb-458a58e026a7","execution_count":46,"outputs":[{"name":"stdout","text":"Model precision: 0.6041666666666666\nModel recall: 0.5370370370370371\nConfusion matrix:\n[[325  38]\n [ 50  58]]\n","output_type":"stream"}]},{"source":"## Random Forest Regressor\n\nJust like in decision trees, we can use random forests for regression as well! It is important to know when to use regression or classification — this usually comes down to what type of variable your target is. Previously, we were using a binary categorical variable (acceptable versus not), so a classification model was used.\n\nWe will now consider a hypothetical new target variable, `usage`, for this data set, which is a continuous variable. We’ve generated some fake drug use amounts in the dataset so that we have numerical values instead of the previous categorical variables.\n\nNow, instead of a classification task, we will use `scikit-learn's RandomForestRegressor()` to carry out a regression task.\n\nNote: Recall that the default evaluation metric for regressors in `scikit-learn` is the R-squared score.","metadata":{},"cell_type":"markdown","id":"6ed8e8f0-499f-49eb-a247-71ee7cef5ef7"},{"source":"usage = np.random.normal(loc=5, size=drugs.shape[0])\ndrugs['Usage'] = usage\ndrugs.Usage.describe()\n\n# drop the binary target variable\ndrugs.drop('Mushrooms', inplace=True, axis=1)","metadata":{},"cell_type":"code","id":"dd9939ed-d7ce-4113-b2bf-6a6cf568882f","execution_count":47,"outputs":[]},{"source":"drugs.head()","metadata":{},"cell_type":"code","id":"7aff13c2-b67d-46d5-abc7-6e5418d5aec7","execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"application/com.datacamp.data-table.v1+json":{"table":{"schema":{"fields":[{"name":"index","type":"integer"},{"name":"Age","type":"integer"},{"name":"Gender","type":"number"},{"name":"Education","type":"integer"},{"name":"Country","type":"number"},{"name":"Ethnicity","type":"number"},{"name":"Nscore","type":"number"},{"name":"Escore","type":"number"},{"name":"Oscore","type":"number"},{"name":"AScore","type":"number"},{"name":"Cscore","type":"number"},{"name":"Impulsive","type":"number"},{"name":"SS","type":"number"},{"name":"Usage","type":"number"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":[{"index":0,"Age":1,"Gender":1,"Education":8,"Country":5,"Ethnicity":6,"Nscore":-0.67825,"Escore":1.93886,"Oscore":1.43533,"AScore":0.76096,"Cscore":-0.14277,"Impulsive":-0.71126,"SS":-0.21575,"Usage":4.5755081914},{"index":1,"Age":2,"Gender":1,"Education":5,"Country":5,"Ethnicity":6,"Nscore":-0.46725,"Escore":0.80523,"Oscore":-0.84732,"AScore":-1.6209,"Cscore":-1.0145,"Impulsive":-1.37983,"SS":0.40148,"Usage":3.8909466192},{"index":2,"Age":0,"Gender":0,"Education":7,"Country":5,"Ethnicity":6,"Nscore":-0.14882,"Escore":-0.80615,"Oscore":-0.01928,"AScore":0.59042,"Cscore":0.58489,"Impulsive":-1.37983,"SS":-1.18084,"Usage":2.2711657678},{"index":3,"Age":2,"Gender":0,"Education":8,"Country":5,"Ethnicity":6,"Nscore":0.73545,"Escore":-1.6334,"Oscore":-0.45174,"AScore":-0.30172,"Cscore":1.30612,"Impulsive":-0.21712,"SS":-0.21575,"Usage":3.6692756746},{"index":4,"Age":5,"Gender":0,"Education":3,"Country":1,"Ethnicity":6,"Nscore":-0.67825,"Escore":-0.30033,"Oscore":-1.55521,"AScore":2.03972,"Cscore":1.63088,"Impulsive":-1.37983,"SS":-1.54858,"Usage":4.1778702363}]},"total_rows":5,"truncation_type":null},"text/plain":"   Age  Gender  Education  Country  ...   Cscore  Impulsive       SS     Usage\n0    1     1.0          8      5.0  ... -0.14277   -0.71126 -0.21575  4.575508\n1    2     1.0          5      5.0  ... -1.01450   -1.37983  0.40148  3.890947\n2    0     0.0          7      5.0  ...  0.58489   -1.37983 -1.18084  2.271166\n3    2     0.0          8      5.0  ...  1.30612   -0.21712 -0.21575  3.669276\n4    5     0.0          3      1.0  ...  1.63088   -1.37983 -1.54858  4.177870\n\n[5 rows x 13 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Gender</th>\n      <th>Education</th>\n      <th>Country</th>\n      <th>Ethnicity</th>\n      <th>Nscore</th>\n      <th>Escore</th>\n      <th>Oscore</th>\n      <th>AScore</th>\n      <th>Cscore</th>\n      <th>Impulsive</th>\n      <th>SS</th>\n      <th>Usage</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1.0</td>\n      <td>8</td>\n      <td>5.0</td>\n      <td>6.0</td>\n      <td>-0.67825</td>\n      <td>1.93886</td>\n      <td>1.43533</td>\n      <td>0.76096</td>\n      <td>-0.14277</td>\n      <td>-0.71126</td>\n      <td>-0.21575</td>\n      <td>4.575508</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1.0</td>\n      <td>5</td>\n      <td>5.0</td>\n      <td>6.0</td>\n      <td>-0.46725</td>\n      <td>0.80523</td>\n      <td>-0.84732</td>\n      <td>-1.62090</td>\n      <td>-1.01450</td>\n      <td>-1.37983</td>\n      <td>0.40148</td>\n      <td>3.890947</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>7</td>\n      <td>5.0</td>\n      <td>6.0</td>\n      <td>-0.14882</td>\n      <td>-0.80615</td>\n      <td>-0.01928</td>\n      <td>0.59042</td>\n      <td>0.58489</td>\n      <td>-1.37983</td>\n      <td>-1.18084</td>\n      <td>2.271166</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>0.0</td>\n      <td>8</td>\n      <td>5.0</td>\n      <td>6.0</td>\n      <td>0.73545</td>\n      <td>-1.63340</td>\n      <td>-0.45174</td>\n      <td>-0.30172</td>\n      <td>1.30612</td>\n      <td>-0.21712</td>\n      <td>-0.21575</td>\n      <td>3.669276</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>1.0</td>\n      <td>6.0</td>\n      <td>-0.67825</td>\n      <td>-0.30033</td>\n      <td>-1.55521</td>\n      <td>2.03972</td>\n      <td>1.63088</td>\n      <td>-1.37983</td>\n      <td>-1.54858</td>\n      <td>4.177870</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"source":"# split the data into 75% train and 25% validation\n# y = drugs.Usage\n# X = drugs.drop('Usage', axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=True, random_state=0)\n\n# fit a random forest regression model\nrfr = RandomForestRegressor()\nrfr.fit(X_train, y_train)\n\nr_squared_train = rfr.score(X_train, y_train)\nprint(f'Train set R^2: {r_squared_train}')\n\nr_squared_test = rfr.score(X_test, y_test)\nprint(f'Test set R^2: {r_squared_test}')","metadata":{},"cell_type":"code","id":"f050e37e-f014-406b-b8ed-e4d06d8e0e9b","execution_count":53,"outputs":[{"name":"stdout","text":"Train set R^2: 0.8565104763893777\nTest set R^2: -0.06913778796660353\n","output_type":"stream"}]},{"source":"avg_usage = np.mean(y)\nprint(f'True average usage: {avg_usage}')\n\ntrain_pred = rfr.predict(X_train)\ntest_pred = rfr.predict(X_test)\nmae_train = mean_absolute_error(y_train, train_pred)\nprint(f'Train MAE: {mae_train}')\nmae_test = mean_absolute_error(y_test, test_pred)\nprint(f'Test MAE: {mae_test}')","metadata":{},"cell_type":"code","id":"26ef7329-1b22-4375-ae14-3086012d80e8","execution_count":61,"outputs":[{"name":"stdout","text":"True average usage: 4.981528955785005\nTrain MAE: 0.30397543751531797\nTest MAE: 0.8895955882291855\n","output_type":"stream"}]},{"source":"## Review\n\nHere are some of the major takeaways about random forests:\n- A random forest is an ensemble machine learning model. It makes a classification by aggregating the classifications of many decision trees.\n- Random forests are used to avoid overfitting. By aggregating the classification of multiple trees, having overfitted trees in a random forest is less impactful.\n- Every decision tree in a random forest is created by using a different subset of data points from the training set. Those data points are chosen at random with replacement, which means a single data point can be chosen more than once. This process is known as bagging.\n- When creating a tree in a random forest, a randomly selected subset of features are considered as candidates for the best splitting feature. If your dataset has `n` features, it is common practice to randomly select the square root of `n` features.","metadata":{},"cell_type":"markdown","id":"20654125-70cf-455f-b6c4-ce5dcfad029d"},{"source":"","metadata":{},"cell_type":"code","id":"2d0b8e47-03dc-43ab-85ab-54b797a1375f","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":""},"kernelspec":{"display_name":"","name":""}},"nbformat":4,"nbformat_minor":5}