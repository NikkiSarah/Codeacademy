{"cells":[{"source":"# **Boosting Machine Learning Models**\n\n## Boosting\n\nIn this module we will cover a powerful ensemble method called Boosting. Boosted ensemble methods use weak learners as base models that are simple and tend to suffer from high bias. The weak learners underfit the data.\n\nBoosting is a sequential learning technique where each of the base models builds off of the previous model. Each subsequent model aims to improve the performance of the final ensemble model by attempting to fix the errors in the previous stage.\n\nThere are two important decisions that need to be made to perform boosted ensembling:\n- Sequential Fitting Method\n- Aggregation Method\n\nTwo boosting algorithms that will be covered in detail in this module are **Adaptive Boosting** and **Gradient Boosting**.\n\nWhile boosting can be applied to any base machine learning algorithm, we will demonstrate with an extremely popular choice as a base estimator, the decision tree. Recall that Decision Trees are a commonly used and powerful machine learning algorithm because they are easy to interpret. Additionally, the training data requires very little manipulation (no need standardization, removal of collinearity, etc.).\n\nThe major limitation to decision trees is that they tend to suffer from high variance and are therefore prone to overfitting. They are good at making a series of decisions which cause them to memorize the training data, so they do not generalize well to unseen data. In the following exercises we will explore how to work past these limitations while using decision trees for boosting.\n\n<img src=\"bag_vs_boost.png\" width=\"40%\" height=\"40%\">","metadata":{},"cell_type":"markdown","id":"384f8125-ee05-47c0-bec6-00d007a19d68"},{"source":"## Adaptive Boosting\n\nAdaptive Boosting (or AdaBoost) is a sequential ensembling method that can be used for both classification and regression. It can use any base machine learning model, though it is most commonly used with decision trees.\n\nFor AdaBoost, the **Sequential Fitting Method** is accomplished by updating the weight attached to each of the training dataset observations as we proceed from one base model to the next. The **Aggregation Method** is a weighted sum of those base models where the model weight is dependent on the error of that particular estimator.\n\nThe training of an AdaBoost model is the process of determining the training dataset observation weights at each step as well as the final weight for each base model for aggregation.\n\n<img src=\"adaboost.png\" width=\"40%\" height=\"40%\">","metadata":{},"cell_type":"markdown","id":"be1d5aa0-6179-48fe-9fb7-7ea73b5caeb7"},{"source":"Let’s take this opportunity to implement AdaBoost on a real dataset and solve a classification problem.\n\nWe will be using a dataset from UCI’s Machine Learning Repository to evaluate drug usage based on a set of demographic characteristics.","metadata":{},"cell_type":"markdown","id":"87ee37f0-fc07-4b3b-9e87-1f144183c4ae"},{"source":"import pandas as pd\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.tree import DecisionTreeClassifier","metadata":{},"cell_type":"code","id":"e1264f6d-643b-4d14-a023-4a861fe1b39e","execution_count":13,"outputs":[]},{"source":"# import the data\ndrugs = pd.read_csv(\"drug_consumption.csv\")\n# filter out unnecessary columns\ndrugs.drop(\"ID\", inplace=True, axis=1)\ndrugs.drop(drugs.iloc[:, 12:27], inplace=True, axis=1)\ndrugs.drop(drugs.iloc[:, -3:], inplace=True, axis=1)\ndrugs.head()","metadata":{},"cell_type":"code","id":"717da2a1-8e16-4b3e-a2d5-e9b9b83307b1","execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"application/com.datacamp.data-table.v1+json":{"table":{"schema":{"fields":[{"name":"index","type":"integer"},{"name":"Age","type":"string"},{"name":"Gender","type":"string"},{"name":"Education","type":"string"},{"name":"Country","type":"string"},{"name":"Ethnicity","type":"string"},{"name":"Nscore","type":"number"},{"name":"Escore","type":"number"},{"name":"Oscore","type":"number"},{"name":"AScore","type":"number"},{"name":"Cscore","type":"number"},{"name":"Impulsive","type":"number"},{"name":"SS","type":"number"},{"name":"Mushrooms","type":"string"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":[{"index":0,"Age":"25-34","Gender":"M","Education":"Doctorate degree","Country":"UK","Ethnicity":"White","Nscore":-0.67825,"Escore":1.93886,"Oscore":1.43533,"AScore":0.76096,"Cscore":-0.14277,"Impulsive":-0.71126,"SS":-0.21575,"Mushrooms":"CL0"},{"index":1,"Age":"35-44","Gender":"M","Education":"Professional certificate/ diploma","Country":"UK","Ethnicity":"White","Nscore":-0.46725,"Escore":0.80523,"Oscore":-0.84732,"AScore":-1.6209,"Cscore":-1.0145,"Impulsive":-1.37983,"SS":0.40148,"Mushrooms":"CL1"},{"index":2,"Age":"18-24","Gender":"F","Education":"Masters degree","Country":"UK","Ethnicity":"White","Nscore":-0.14882,"Escore":-0.80615,"Oscore":-0.01928,"AScore":0.59042,"Cscore":0.58489,"Impulsive":-1.37983,"SS":-1.18084,"Mushrooms":"CL0"},{"index":3,"Age":"35-44","Gender":"F","Education":"Doctorate degree","Country":"UK","Ethnicity":"White","Nscore":0.73545,"Escore":-1.6334,"Oscore":-0.45174,"AScore":-0.30172,"Cscore":1.30612,"Impulsive":-0.21712,"SS":-0.21575,"Mushrooms":"CL2"},{"index":4,"Age":"65+","Gender":"F","Education":"Left school at 18 years","Country":"Canada","Ethnicity":"White","Nscore":-0.67825,"Escore":-0.30033,"Oscore":-1.55521,"AScore":2.03972,"Cscore":1.63088,"Impulsive":-1.37983,"SS":-1.54858,"Mushrooms":"CL0"}]},"total_rows":5,"truncation_type":null},"text/plain":"     Age Gender  ...       SS Mushrooms\n0  25-34      M  ... -0.21575       CL0\n1  35-44      M  ...  0.40148       CL1\n2  18-24      F  ... -1.18084       CL0\n3  35-44      F  ... -0.21575       CL2\n4    65+      F  ... -1.54858       CL0\n\n[5 rows x 13 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Gender</th>\n      <th>Education</th>\n      <th>Country</th>\n      <th>Ethnicity</th>\n      <th>Nscore</th>\n      <th>Escore</th>\n      <th>Oscore</th>\n      <th>AScore</th>\n      <th>Cscore</th>\n      <th>Impulsive</th>\n      <th>SS</th>\n      <th>Mushrooms</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>25-34</td>\n      <td>M</td>\n      <td>Doctorate degree</td>\n      <td>UK</td>\n      <td>White</td>\n      <td>-0.67825</td>\n      <td>1.93886</td>\n      <td>1.43533</td>\n      <td>0.76096</td>\n      <td>-0.14277</td>\n      <td>-0.71126</td>\n      <td>-0.21575</td>\n      <td>CL0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>35-44</td>\n      <td>M</td>\n      <td>Professional certificate/ diploma</td>\n      <td>UK</td>\n      <td>White</td>\n      <td>-0.46725</td>\n      <td>0.80523</td>\n      <td>-0.84732</td>\n      <td>-1.62090</td>\n      <td>-1.01450</td>\n      <td>-1.37983</td>\n      <td>0.40148</td>\n      <td>CL1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>18-24</td>\n      <td>F</td>\n      <td>Masters degree</td>\n      <td>UK</td>\n      <td>White</td>\n      <td>-0.14882</td>\n      <td>-0.80615</td>\n      <td>-0.01928</td>\n      <td>0.59042</td>\n      <td>0.58489</td>\n      <td>-1.37983</td>\n      <td>-1.18084</td>\n      <td>CL0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>35-44</td>\n      <td>F</td>\n      <td>Doctorate degree</td>\n      <td>UK</td>\n      <td>White</td>\n      <td>0.73545</td>\n      <td>-1.63340</td>\n      <td>-0.45174</td>\n      <td>-0.30172</td>\n      <td>1.30612</td>\n      <td>-0.21712</td>\n      <td>-0.21575</td>\n      <td>CL2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>65+</td>\n      <td>F</td>\n      <td>Left school at 18 years</td>\n      <td>Canada</td>\n      <td>White</td>\n      <td>-0.67825</td>\n      <td>-0.30033</td>\n      <td>-1.55521</td>\n      <td>2.03972</td>\n      <td>1.63088</td>\n      <td>-1.37983</td>\n      <td>-1.54858</td>\n      <td>CL0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"source":"# map the categorical variables into a numeric representation\nage_map = {\"18-24\": 0, \"25-34\": 1, \"35-44\": 2, \"45-54\": 3, \"55-64\": 4, \"65+\": 5}\neduc_map = {\"Left school before 16 years\": 0, \"Left school at 16 years\": 1, \"Left school at 17 years\": 2, \"Left school at 18 years\": 3,\n            \"Some college or university, no certificate or degree\": 4, \"Professional certificate/ diploma\": 5, \"University degree\": 6, \n            \"Masters degree\": 7, \"Doctorate degree\": 8}\nuser_map = {'CL0': 0, 'CL1': 0, 'CL2': 0, 'CL3': 1, 'CL4': 1, 'CL5': 1, 'CL6': 1}\n\ndicts = [age_map, educ_map, user_map]\nfor col, enc in zip(drugs[['Age', 'Education', 'Mushrooms']], dicts):\n    drugs[col] = drugs[col].map(lambda x: enc.get(x, x))\n\nenc = OrdinalEncoder()\nencoded_cols = enc.fit_transform(drugs[['Gender', 'Country', 'Ethnicity']])\ndrugs[['Gender', 'Country', 'Ethnicity']] = encoded_cols\ndrugs.head()","metadata":{},"cell_type":"code","id":"71a37f60-1c1b-473a-a443-7a1f6ffebb9d","execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"application/com.datacamp.data-table.v1+json":{"table":{"schema":{"fields":[{"name":"index","type":"integer"},{"name":"Age","type":"integer"},{"name":"Gender","type":"number"},{"name":"Education","type":"integer"},{"name":"Country","type":"number"},{"name":"Ethnicity","type":"number"},{"name":"Nscore","type":"number"},{"name":"Escore","type":"number"},{"name":"Oscore","type":"number"},{"name":"AScore","type":"number"},{"name":"Cscore","type":"number"},{"name":"Impulsive","type":"number"},{"name":"SS","type":"number"},{"name":"Mushrooms","type":"integer"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":[{"index":0,"Age":1,"Gender":1,"Education":8,"Country":5,"Ethnicity":6,"Nscore":-0.67825,"Escore":1.93886,"Oscore":1.43533,"AScore":0.76096,"Cscore":-0.14277,"Impulsive":-0.71126,"SS":-0.21575,"Mushrooms":0},{"index":1,"Age":2,"Gender":1,"Education":5,"Country":5,"Ethnicity":6,"Nscore":-0.46725,"Escore":0.80523,"Oscore":-0.84732,"AScore":-1.6209,"Cscore":-1.0145,"Impulsive":-1.37983,"SS":0.40148,"Mushrooms":0},{"index":2,"Age":0,"Gender":0,"Education":7,"Country":5,"Ethnicity":6,"Nscore":-0.14882,"Escore":-0.80615,"Oscore":-0.01928,"AScore":0.59042,"Cscore":0.58489,"Impulsive":-1.37983,"SS":-1.18084,"Mushrooms":0},{"index":3,"Age":2,"Gender":0,"Education":8,"Country":5,"Ethnicity":6,"Nscore":0.73545,"Escore":-1.6334,"Oscore":-0.45174,"AScore":-0.30172,"Cscore":1.30612,"Impulsive":-0.21712,"SS":-0.21575,"Mushrooms":0},{"index":4,"Age":5,"Gender":0,"Education":3,"Country":1,"Ethnicity":6,"Nscore":-0.67825,"Escore":-0.30033,"Oscore":-1.55521,"AScore":2.03972,"Cscore":1.63088,"Impulsive":-1.37983,"SS":-1.54858,"Mushrooms":0}]},"total_rows":5,"truncation_type":null},"text/plain":"   Age  Gender  Education  Country  ...   Cscore  Impulsive       SS  Mushrooms\n0    1     1.0          8      5.0  ... -0.14277   -0.71126 -0.21575          0\n1    2     1.0          5      5.0  ... -1.01450   -1.37983  0.40148          0\n2    0     0.0          7      5.0  ...  0.58489   -1.37983 -1.18084          0\n3    2     0.0          8      5.0  ...  1.30612   -0.21712 -0.21575          0\n4    5     0.0          3      1.0  ...  1.63088   -1.37983 -1.54858          0\n\n[5 rows x 13 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Gender</th>\n      <th>Education</th>\n      <th>Country</th>\n      <th>Ethnicity</th>\n      <th>Nscore</th>\n      <th>Escore</th>\n      <th>Oscore</th>\n      <th>AScore</th>\n      <th>Cscore</th>\n      <th>Impulsive</th>\n      <th>SS</th>\n      <th>Mushrooms</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1.0</td>\n      <td>8</td>\n      <td>5.0</td>\n      <td>6.0</td>\n      <td>-0.67825</td>\n      <td>1.93886</td>\n      <td>1.43533</td>\n      <td>0.76096</td>\n      <td>-0.14277</td>\n      <td>-0.71126</td>\n      <td>-0.21575</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1.0</td>\n      <td>5</td>\n      <td>5.0</td>\n      <td>6.0</td>\n      <td>-0.46725</td>\n      <td>0.80523</td>\n      <td>-0.84732</td>\n      <td>-1.62090</td>\n      <td>-1.01450</td>\n      <td>-1.37983</td>\n      <td>0.40148</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>7</td>\n      <td>5.0</td>\n      <td>6.0</td>\n      <td>-0.14882</td>\n      <td>-0.80615</td>\n      <td>-0.01928</td>\n      <td>0.59042</td>\n      <td>0.58489</td>\n      <td>-1.37983</td>\n      <td>-1.18084</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>0.0</td>\n      <td>8</td>\n      <td>5.0</td>\n      <td>6.0</td>\n      <td>0.73545</td>\n      <td>-1.63340</td>\n      <td>-0.45174</td>\n      <td>-0.30172</td>\n      <td>1.30612</td>\n      <td>-0.21712</td>\n      <td>-0.21575</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>1.0</td>\n      <td>6.0</td>\n      <td>-0.67825</td>\n      <td>-0.30033</td>\n      <td>-1.55521</td>\n      <td>2.03972</td>\n      <td>1.63088</td>\n      <td>-1.37983</td>\n      <td>-1.54858</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"source":"# split the data into 70% train and 30% validation\ny = drugs.Mushrooms\nX = drugs.drop('Mushrooms', axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, shuffle=True, stratify=y, random_state=0)\n\n# create a base classifier in the form of a decision stump (a decision tree with two leaf nodes)\ndecision_stump = DecisionTreeClassifier(max_depth=1)\nprint(decision_stump.get_params())","metadata":{},"cell_type":"code","id":"9ccf1caf-a048-49ed-b404-039afb976c8e","execution_count":8,"outputs":[{"name":"stdout","text":"{'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 1, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': None, 'splitter': 'best'}\n","output_type":"stream"}]},{"source":"# create an adaboost classification model with the decision stump as the base classifier and 5 trees\nada_clf = AdaBoostClassifier(base_estimator=decision_stump, n_estimators=5)\nprint(ada_clf.get_params())","metadata":{},"cell_type":"code","id":"f3369aa0-ff7e-4448-ac26-355b0c6fa3a9","execution_count":9,"outputs":[{"name":"stdout","text":"{'algorithm': 'SAMME.R', 'base_estimator__ccp_alpha': 0.0, 'base_estimator__class_weight': None, 'base_estimator__criterion': 'gini', 'base_estimator__max_depth': 1, 'base_estimator__max_features': None, 'base_estimator__max_leaf_nodes': None, 'base_estimator__min_impurity_decrease': 0.0, 'base_estimator__min_samples_leaf': 1, 'base_estimator__min_samples_split': 2, 'base_estimator__min_weight_fraction_leaf': 0.0, 'base_estimator__random_state': None, 'base_estimator__splitter': 'best', 'base_estimator': DecisionTreeClassifier(max_depth=1), 'learning_rate': 1.0, 'n_estimators': 5, 'random_state': None}\n","output_type":"stream"}]},{"source":"# fit the model to the training data\nada_clf.fit(X_train, y_train)\n# make predictions on the test data\ny_pred = ada_clf.predict(X_test)\n\n# assess model performance\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(f'Accuracy: {accuracy}')\nprint(f'Precision: {precision}')\nprint(f'Recall: {recall}')\nprint(f'F1 score: {f1}')","metadata":{},"cell_type":"code","id":"3bb9220b-b011-4e21-b8c0-a0bdf08d64b3","execution_count":10,"outputs":[{"name":"stdout","text":"Accuracy: 0.8091872791519434\nPrecision: 0.5948275862068966\nRecall: 0.5307692307692308\nF1 score: 0.5609756097560975\n","output_type":"stream"}]},{"source":"# view the confusion matrix\ntest_conf_matrix = pd.DataFrame(\n    confusion_matrix(y_test, y_pred, labels=[1, 0]), \n    index=['actual yes', 'actual no'], \n    columns=['predicted yes', 'predicted no']\n)\nprint(f'Confusion Matrix:\\n{test_conf_matrix.to_string()}')","metadata":{},"cell_type":"code","id":"5273934c-a7fd-40d8-a8f2-488bd1151615","execution_count":11,"outputs":[{"name":"stdout","text":"Confusion Matrix:\n            predicted yes  predicted no\nactual yes             69            61\nactual no              47           389\n","output_type":"stream"}]},{"source":"## Gradient Boosting\n\nGradient Boosting is a sequential ensembling method that can be used for both classification and regression. It can use any base machine learning model, though it is most commonly used with decision trees, known as Gradient Boosted Trees.\n\nFor Gradient Boost, the **Sequential Fitting Method** is accomplished by fitting a base model to the negative gradient of the error in the previous stage. The **Aggregation Method** is a weighted sum of those base models where the model weight is constant.\n\nThe training of a Gradient Boosted model is the process of determining the base model error at each step and using those to determine how to best formulate the subsequent base model.\n\n<img src=\"gbm.png\" width=\"40%\" height=\"40%\">","metadata":{},"cell_type":"markdown","id":"fd6e97db-6a8a-4780-932a-1dd8c4379821"},{"source":"While Gradient Boosting can be applied to any base machine learning model, decision trees are commonly used in practice. In this example we will be focusing on a Gradient Boosted Trees model.\n\nOur first step is to fit an estimator, the 1st Base Model. Recall that the base estimators for boosting algorithms tend to be simple and high bias. In contrast to AdaBoost which leveraged the simplest form of decision trees, the decision stump with only 1 level, gradient boosted trees can and actually do tend to include a few more decision branches. Often gradient boosted trees will have up to 32 leaf nodes, which corresponds to a tree depth of 5 levels. In this example, we are limiting the depth of the base estimators to 2, corresponding to 4 leaf nodes.\n\nOnce the 1st Base Model is trained, the residual errors (`h_1`), of the model given the training training data are determined. The residual error is the difference between the actual and predicted values for each of the training data instances.\n\n$$h_1=y_{actual} - y_{1(predicted)}$$\n\nThe errors will be greater for the training data instances where the model did not do as good of a job with its prediction and will be lower on training data instances where the model fit the data well.\n\nIn the next stage of the sequential learning process, we fit the 2nd Base Model. Here is where the interesting part comes in. Instead of fitting the model to the target values `y_actual` as we are typically used to doing in machine learning, we actually fit the model on the errors of the previous stage, in this case `h_1`. The 2nd Base Model is literally learning from the mistakes of the 1st Base Model through those residuals that were calculated.\n\nThe results of the 2nd Base Model are multiplied by a constant learning rate, `alpha`, and added to the results of the 1st Base Model to give the set of updated predictions. The results of the second base model, which was tasked with fitting the errors of the first base model are multiplied by a constant learning rate, alpha and added to the results of the first base model to give us a set of updated predictions, `y_2(predicted)`.\n\nThe residual errors of the 2nd stage are calculated using the updated predictions to get,\n\n$$h_2=y_{actual} - y_{2(predicted)}$$\n\nThe subsequent stages repeat the same steps. At stage `N`, the base model is fit on the errors calculated at the previous stage `h_(N-1)`. The new model that is fit is multiplied by the constant learning rate `alpha` and added to the predictions of the previous stage.\n\nOnce we have reached the predefined number of estimators for our Gradient Boosting model or the residual errors are not changing between iterations, the model will stop training and we end up with the resultant ensemble model.","metadata":{},"cell_type":"markdown","id":"f05ea50b-bf5e-422e-b3b8-90361f8f8994"},{"source":"# create a gradient boosting classification model with 15 trees\ngb_clf = GradientBoostingClassifier(n_estimators=15)\nprint(gb_clf.get_params())","metadata":{},"cell_type":"code","id":"02a72658-2822-4b26-89c0-20e33b7ae5d1","execution_count":18,"outputs":[{"name":"stdout","text":"{'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'log_loss', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 15, 'n_iter_no_change': None, 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n","output_type":"stream"}]},{"source":"# fit the model to the training data\ngb_clf.fit(X_train, y_train)\n# make predictions on the test data\ny_pred = gb_clf.predict(X_test)\n\n# assess model performance\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(f'Accuracy: {accuracy}')\nprint(f'Precision: {precision}')\nprint(f'Recall: {recall}')\nprint(f'F1 score: {f1}')","metadata":{},"cell_type":"code","id":"da8802d4-c9ad-4c58-ab8e-7f88ced58490","execution_count":19,"outputs":[{"name":"stdout","text":"Accuracy: 0.8021201413427562\nPrecision: 0.5957446808510638\nRecall: 0.4307692307692308\nF1 score: 0.5\n","output_type":"stream"}]},{"source":"# view the confusion matrix\ntest_conf_matrix = pd.DataFrame(\n    confusion_matrix(y_test, y_pred, labels=[1, 0]), \n    index=['actual yes', 'actual no'], \n    columns=['predicted yes', 'predicted no']\n)\nprint(f'Confusion Matrix:\\n{test_conf_matrix.to_string()}')","metadata":{},"cell_type":"code","id":"ea73baf1-08a7-4d33-bf9c-f9b55a1a068b","execution_count":20,"outputs":[{"name":"stdout","text":"Confusion Matrix:\n            predicted yes  predicted no\nactual yes             56            74\nactual no              38           398\n","output_type":"stream"}]},{"source":"","metadata":{},"cell_type":"code","id":"88e62dec-80e3-42b8-9d74-ad6695715fc8","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":""},"kernelspec":{"display_name":"","name":""}},"nbformat":4,"nbformat_minor":5}